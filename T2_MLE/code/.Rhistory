# M2: the Null model with 1 free parameter. We know from Tuesday that the likelihood is maximised when p=0.7. Hence the maximised likelihood under M1 is
L_0.7 <- flip_coin(0.7,10,7)
log_L_0.7 <- log(L_0.7)
# LRT statistic
D <- 2*(log_L_0.7 - log_L_0.5)
dnorm(x = 1.96,mean = 0,sd = 1)
dnorm(x = -1.96:1.96,mean = 0,sd = 1)
sum(dnorm(x = -1.96:1.96,mean = 0,sd = 1))
pnorm(mean = 0,sd = 1,q = 1.96)
pnorm(q = 1.96) - pnorm(-1.96)
sum(dnorm(x = -1.96:1.96,mean = 0,sd = 1))
integrate(dnorm,lower = -1.96, upper = 1.96)
# r: generate random number in specific distribution
rnorm(n = 1)
# r: generate random number in specific distribution
rpois(4)
# r: generate random number in specific distribution
rpois(4, lambda = 3)
# r: generate random number in specific distribution
rnorm(20)
# d:pmf/pdf
pnorm(1:3)
# d:pmf/pdf
dnorm(1:3)
# q:quantile
qnorm()
# q:quantile
qnorm(3)
# q:quantile
qnorm(2)
# q:quantile
qnorm(0.3)
sum(dnorm(-1.96:1.96))
## 2>
# M1: the simplified model with p = 0.5(p is fixed), Under M1, there is 0 free parameter. The maximised likelihood under M1 is
L_1 <- flip_coin(0.5,50,35)
log_L_1 <- log(L_1)
# M2: the Null model with 1 free parameter. We know from Tuesday that the likelihood is maximised when p=0.7. Hence the maximised likelihood under M1 is
L_2 <- flip_coin(0.7,50,35)
log_L_2 <- log(L_2)
# LRT statistic
D <- 2*(log_L_2 - log_L_1)
D
L_1
log_L_1 <- log(L_1)
log_L_1
L_2
log_L_2
# LRT statistic
D <- 2*(log_L_2 - log_L_1)
D
setwd("~/Documents/CMEE_T2/MLE/code")
# Question 4
flower <- read.table("../data/flowering.txt", header = T)
names(flower)
View(flower)
## visualize the data
par(mfrow = c(1,2))
plot(flower$Flowers, flower$State)
plot(flower$Root, flower$State)
## define the logistic log likelihood function
logistic <- function(parm,dat){
a <- parm[1]
b <- parm[2]
c <- parm[3]
state <- dat[,1]
flowers <- dat[,2]
root <- dat[,3]
# model the success probability, via expit transformation
p <- exp(a+b*flowers+c*root) / (1+exp(a+b*flowers+c*root))
# the log likelihood function
log.like <- sum(state*log(p)+(1-state)*log(1-p))
return(log.like)
}
# try
logistic(c(0,0,0), dat = flower)
##############
b <- seq(1,4,0.1)
sigma <- seq(2,5,0.1)
# the lig.likelihood value is stroed in a  matrix
log.likelihood.value
# the lig.likelihood value is stroed in a  matrix
log.likelihood.value<- matrix(nr=length(b), nc=length(sigma))
# Question 4
## read the data
flower <- read.table("../data/flowering.txt", header = T)
plot(dbinom(prob = p, log = TRUE))
plot(dbinom(prob = p, log = TRUE, x = 10,size = 50))
dev.off()
## 3> find the 95% CI
# D = 2*(ln(L2)-ln(L1)) < $\chi^2|1 = 3.84$
# so ln(L2) - ln(L1) < 1.92
p = seq(0,1,0.1)
plot(dbinom(prob = p, log = TRUE, x = 10,size = 50))
plot(p, dbinom(prob = p, log = TRUE, x = 10,size = 50))
plot(p, dbinom(prob = p, log = TRUE, x = 10,size = 100))
plot(p, dbinom(prob = p, log = TRUE, x = 100,size = 100))
plot(p, dbinom(prob = p, log = FALSE, x = 100,size = 100))
plot(p, dbinom(prob = p, log = FALSE, x = 100,size = 10))
plot(p, dbinom(prob = p, log = FALSE, x = 7,size = 10))
plot(p, dbinom(prob = p, log = TRUE, x = 7,size = 10))
log.like <- dbinom(prob = p, log = TRUE, x = 7,size = 10)
plot(p, log.like-max(log.like))
abline(v = -1.92)
abline(h = -1.92)
## 3> find the 95% CI
# D = 2*(ln(L2)-ln(L1)) < $\chi^2|1 = 3.84$
# so ln(L2) - ln(L1) < 1.92
crtic.value <- 3.84/2
abline(h = -crtic.value, v = p[which(log.like == -crtic.value)])
p[which(log.like == -crtic.value)]
which(log.like == -crtic.value)
log.like == -crtic.value
log.like <- dbinom(prob = p, log = TRUE, x = 7,size = 10)
dbinom(prob = p, log = TRUE, x = 7,size = 10)
abline(h = -crtic.value, v = p[which(log.like-max(log.like) == -crtic.value)])
log.like-max(log.like) == -crtic.value
p = seq(0,1,0.01)
log.like <- dbinom(prob = p, log = TRUE, x = 7,size = 10)
plot(p, log.like-max(log.like))
plot(p, log.like-max(log.like),type = 'l')
abline(h = -crtic.value, v = p[which(log.like-max(log.like) == -crtic.value)])
abline(h = -crtic.value, v = c(0.39, 0.92))
abline(h = -crtic.value, v = c(0.39, 0.92), lty = 'dash')
## 3> find the 95% CI
# D = 2*(ln(L2)-ln(L1)) < $\chi^2|1 = 3.84$
# so ln(L2) - ln(L1) < 1.92
critic.value <- 3.84/2
critic.value
# visualize
p = seq(0,1,0.01)
log.like <- dbinom(prob = p, log = TRUE, x = 7,size = 10)
plot(p, log.like-max(log.like),type = 'l')
abline(h = -crtic.value, v = c(0.39, 0.92))
log.like <- dbinom(prob = p, log = TRUE, x = 35,size = 100)
plot(p, log.like-max(log.like),type = 'l')
abline(h = -crtic.value, v = c(0.39, 0.92))
abline(v = c(0.39, 0.92))
# M2: the Null model with 1 free parameter.
# We know from Tuesday that the likelihood is maximised when p=0.7.
# Hence the maximised likelihood under M1 is
L_2 <- flip_coin(0.7,50,35)
L_2
log_L_2 <- log(L_2)
log_L_2
# LRT statistic
D <- 2*(log_L_2 - log_L_1)
D
log_L_1
# visualize
p = seq(0,1,0.01)
log.like <- dbinom(prob = p, log = TRUE, x = 35,size = 100)
plot(p, log.like-max(log.like),type = 'l')
abline(h = -crtic.value)
abline(v = c(0.39, 0.92))
# visualize
p = seq(0,1,0.01)
log.like <- dbinom(prob = p, log = TRUE, x = 7,size = 10)
plot(p, log.like-max(log.like),type = 'l')
abline(h = -crtic.value)
abline(v = c(0.39, 0.92))
log_L_2
# LRT statistic
D <- 2*(log_L_2 - log_L_1)
D
critic.value
max(log.like)
log.like-max(log.like)
log.like > max(log.like)-critic.value
p[log.like > max(log.like)-critic.value]
# so the 95% CI is
within <- p[log.like > max(log.like)-critic.value]
CI <- c(min(within), max(within))
CI
# visualize
p = seq(0,1,0.001)
log.like <- dbinom(prob = p, log = TRUE, x = 7,size = 10)
plot(p, log.like-max(log.like),type = 'l')
abline(h = -crtic.value)
abline(v = c(0.39, 0.92))
# so the 95% CI is
within <- p[log.like > max(log.like)-critic.value]
CI <- c(min(within), max(within))
CI
# visualize
p = seq(0,1,0.001)
log.like <- dbinom(prob = p, log = TRUE, x = 35,size = 100)
plot(p, log.like-max(log.like),type = 'l')
abline(h = -crtic.value)
log.like <- dbinom(prob = p, log = TRUE, x = 35,size = 100)
plot(p, log.like-max(log.like),type = 'l')
abline(h = -crtic.value)
# so the 95% CI is
within <- p[log.like > max(log.like)-critic.value]
CI <- c(min(within), max(within))
CI
abline(v = CI)
# visualize
p = seq(0,1,0.001)
log.like <- dbinom(prob = p, log = TRUE, x = 35,size = 50)
plot(p, log.like-max(log.like),type = 'l')
abline(h = -crtic.value)
# so the 95% CI is
within <- p[log.like > max(log.like)-critic.value]
CI <- c(min(within), max(within))
CI
abline(v = CI)
# calculate the chi square
chi.sq <- qchisq(p = 0.95, df = 1)
# df of this test is 1-0=1, critical value of chi-square is
chi.sq
# df of this test is 1-0=1, critical value of chi-square is
qchisq(p = 0.95,df = 1)
## 3> find the 95% CI
# D = 2*(ln(L2)-ln(L1)) < $\chi^2|1 = 3.84$
# so ln(L2) - ln(L1) < 1.92
critic.value <- chi.sq/2
critic.value
########################## Wednesday ##########################
# Question 3 likelihood ratio test
up(a)
########################## Wednesday ##########################
# Question 3 likelihood ratio test
upper(a)
density(dnorm(-1.96:1.96))
density(1:4)
########################## Thursday ##########################
# EXAMPLE IN CLASS: CI, linear regression
## EXAMPLE on Wednesday: Linear regression test for intercept
# M1(without an intercept), M2(with intercept), M1 is nested in M2
regression.no.intercept.log.likelihood <- function(parm, dat){
b <- parm[1]
sigma <- parm[2]
x <- dat[,1]
y <- dat[,2]
error.term <- (y-b*x)
density <- dnorm(error.term, mean = 0, sd = sigma, log = TRUE)
# log likelihood is the sum of the densities
return(sum(density))
}
# performing likelihood-ratio test
M1 <- optim(par = c(1,1), regression.no.intercept.log.likelihood,
dat = recap.data, method = "L-BFGS-B",
lower = c(-1000,0.0001), upper = c(1000,10000),
control = list(fnscale = -1), hessian = T)
# check optimize
optimize(binomial.likelihood, interval = c(0,1), maximum = TRUE)
binomial.likelihood <- function(p){
choose(10,7)*p^7*(1-p)^3
}
# calculate the likelihood value at 9 = 0.1
binomial.likelihood(p=0.1)
# plot the likelihood function for a range of P
p <- seq(0, 1, 0.01)
likelihood.value <- binomial.likelihood(p)
plot(p, likelihood.value, type = 'l')
# plot the log likelihood function
log.binomial.likelihood <- function(p){
log(binomial.likelihood(p = p))
}
# plot the log-likelihood
p <- seq(0,1,0.01)
log.likelihood.value <- log.binomial.likelihood(p)
plot(p, log.likelihood.value, type = 'l')
# check optimize
optimize(binomial.likelihood, interval = c(0,1), maximum = TRUE)
# performing likelihood-ratio test
M1 <- optim(par = c(1,1), regression.no.intercept.log.likelihood,
dat = recap.data, method = "L-BFGS-B",
lower = c(-1000,0.0001), upper = c(1000,10000),
control = list(fnscale = -1), hessian = T)
############ Rabbit Example ############
# read the data
recap.data <- read.csv("../data/recapture.csv", header = TRUE)
# performing likelihood-ratio test
M1 <- optim(par = c(1,1), regression.no.intercept.log.likelihood,
dat = recap.data, method = "L-BFGS-B",
lower = c(-1000,0.0001), upper = c(1000,10000),
control = list(fnscale = -1), hessian = T)
M2 <- optim(par = c(1,1,1), regression.log.likelihood,
dat = recap.data, method = "L-BFGS-B",
lower = c(-1000,-1000,0.0001), upper = c(1000,1000,10000),
control = list(fnscale = -1), hessian = T)
# 1. log-likelihood
regression.log.likelihood <- function(parm, dat){
a <- parm[1]
b <- parm[2]
sigma <- parm[3]
x <- dat[,1]
y <- dat[,2]
density <- dnorm(y, mean = a+b*x, sd = sigma, log = T)
# the log-likelihood is the sum of individual log-density
return(sum(density))
}
M2 <- optim(par = c(1,1,1), regression.log.likelihood,
dat = recap.data, method = "L-BFGS-B",
lower = c(-1000,-1000,0.0001), upper = c(1000,1000,10000),
control = list(fnscale = -1), hessian = T)
# calculate the test statistic D
D <- 2*(M1$value-M2$value)
D
# calculate the test statistic D
D <- 2*(M2$value-M1$value)
D
# calculate the critical value
qchisq(0.95,1)
# calculate the critical value
chisq <- qchisq(0.95,1)
D < chisq
##########################
# Example on Wednesday: Non-constant variance regression
regression.non.constant.var.log.likelihood(parm, dat){
b <- parm[1]
sigma <- parm[2]
x <- dat[,1]
y <- dat[,2]
error.term <- y-b*x
density <- dnorm(error.term, mean = 0, sd = x*sigma, log = TRUE)
return(sum(density))
}
##########################
# Example on Wednesday: Non-constant variance regression
regression.non.constant.var.log.likelihood(parm, dat){
b <- parm[1]
sigma <- parm[2]
x <- dat[,1]
y <- dat[,2]
error.term <- y-b*x
density <- dnorm(error.term, mean = 0, sd = x*sigma, log = TRUE)
return(sum(density))
}
##########################
# Example on Wednesday: Non-constant variance regression
regression.non.constant.var.log.likelihood(parm, dat){
b <- parm[1]
sigma <- parm[2]
x <- dat[,1]
y <- dat[,2]
error.term <- y-b*x
density <- dnorm(error.term, mean = 0, sd = x*sigma, log = TRUE)
return(sum(density))
}
##########################
# Example on Wednesday: Non-constant variance regression
regression.non.constant.var.log.likelihood(parm, dat){
##########################
# Example on Wednesday: Non-constant variance regression
regression.non.constant.var.log.likelihood <- function(parm, dat){
b <- parm[1]
sigma <- parm[2]
x <- dat[,1]
y <- dat[,2]
error.term <- y-b*x
density <- dnorm(error.term, mean = 0, sd = x*sigma, log = TRUE)
return(sum(density))
}
# maximise the log-likelihood
M4 <- optim(par = c(1,1), regression.non.constant.var.log.likelihood,
dat = recap.data, method = "L-BFGS-B",
lower = c(-1000,0.0001), upper = c(1000,10000),
control = list(fnscale = -1))
M4
# optimise the log-likelihood funcion
optim(par = c(1,1,1), regression.log.likelihood, method = 'L-BFGS-B',
lower = c(-1000,-1000,0.0001), upper = c(1000,1000,10000),
control = list(fnscale=-1), dat = recap.data, hessian = T)
D < chisq
D
# 1. log-likelihood
regression.log.likelihood <- function(parm, dat){
a <- parm[1]
b <- parm[2]
sigma <- parm[3]
x <- dat[,1]
y <- dat[,2]
density <- dnorm(y, mean = a+b*x, sd = sigma, log = T)
# the log-likelihood is the sum of individual log-density
return(sum(density))
}
# 2. log-likelihood + error
regression.log.likelihood.error <- function(parm, dat){
a <- parm[1]
b <- parm[2]
sigma <- parm[3]
x <- dat[,1]
y <- dat[,2]
error.term <- (y-a-b*x)
density <- dnorm(error.term, mean = 0, sd = sigma, log = T)
return(sum(density))
}
# 2. log-likelihood + error
regression.log.likelihood.error <- function(parm, dat){
a <- parm[1]
b <- parm[2]
sigma <- parm[3]
x <- dat[,1]
y <- dat[,2]
error.term <- (y-a-b*x)
density <- dnorm(error.term, mean = 0, sd = sigma, log = T)
return(sum(density))
}
regression.log.likelihood(c(1,1,1), dat = recap.data)
regression.log.likelihood.error(c(1,1,1), dat = recap.data)
# optimise the log-likelihood funcion
optim(par = c(1,1,1), regression.log.likelihood, method = 'L-BFGS-B',
lower = c(-1000,-1000,0.0001), upper = c(1000,1000,10000),
control = list(fnscale=-1), dat = recap.data, hessian = T)
# perform the same analysis with lm()
m <- lm(length_diff ~ day, data = recap.data)
summary(m)
n <- nrow(recap.data)
sqrt(var(m$residual)*(n-1)/n)
########################## Wednesday ##########################
## EXAMPLE: Linear regression test for intercept
# M1(without an intercept), M2(with intercept), M1 is nested in M2
# define the function without intercept for M1
regression.no.intercept.log.likelihood <- function(parm, dat){
b <- parm[1]
sigma <- parm[2]
x <- dat[,1]
y <- dat[,2]
error.term <- (y-b*x)
density <- dnorm(error.term, mean = 0, sd = sigma, log = TRUE)
# log likelihood is the sum of the densities
return(sum(density))
}
# performing likelihood-ratio test
M1 <- optim(par = c(1,1), regression.no.intercept.log.likelihood,
dat = recap.data, method = "L-BFGS-B",
lower = c(-1000,0.0001), upper = c(1000,10000),
control = list(fnscale = -1), hessian = T)
M2 <- optim(par = c(1,1,1), regression.log.likelihood,
dat = recap.data, method = "L-BFGS-B",
lower = c(-1000,-1000,0.0001), upper = c(1000,1000,10000),
control = list(fnscale = -1), hessian = T)
# calculate the test statistic D
D <- 2*(M2$value-M1$value)
D
# calculate the critical value
chisq <- qchisq(0.95,1)
D < chisq
##########################
# Example: Non-constant variance regression
regression.non.constant.var.log.likelihood <- function(parm, dat){
b <- parm[1]
sigma <- parm[2]
x <- dat[,1]
y <- dat[,2]
error.term <- y-b*x
density <- dnorm(error.term, mean = 0, sd = x*sigma, log = TRUE)
return(sum(density))
}
# maximise the log-likelihood
M4 <- optim(par = c(1,1), regression.non.constant.var.log.likelihood,
dat = recap.data, method = "L-BFGS-B",
lower = c(-1000,0.0001), upper = c(1000,10000),
control = list(fnscale = -1))
M4
# compute the log.likelihood value for each pair of parameters
for (i in 1:length(b)) {
for (j in 1:length(sigma)) {
log.like.value[i,j] <-
regression.no.intercept.log.likelihood
(parm = c(b[i],sigma[j]), dat = recap.data)
}
}
# compute the log.likelihood value for each pair of parameters
for (i in 1:length(b)) {
for (j in 1:length(sigma)) {
log.like.value[i,j] <-
regression.no.intercept.log.likelihood(parm = c(b[i],sigma[j]), dat = recap.data)
}
}
# the log.likelihood value is stroed in a matrix
log.like.value <- matrix(nr=length(b), nc=length(sigma))
# compute the log.likelihood value for each pair of parameters
for (i in 1:length(b)) {
for (j in 1:length(sigma)) {
log.like.value[i,j] <-
regression.no.intercept.log.likelihood(parm = c(b[i],sigma[j]), dat = recap.data)
}
}
rel.log.like.value <- log.like.value - M1$value
persp(b, sigma, rel.log.like.value, theta = 30, phi = 20,
xlab = "b", ylab = "sigma", zlab = "rel.log.likelihood.value",
col = "grey")
# draw a cross to indicate the maximum
points(M1$par[1], M1$par[2],pch=3)
# How about a contour plot>
contour(b,sigma,rel.log.like.value,xlab='b',
ylab='sigma',xlim = c(2.5,3.9),ylim = c(2.0,4.3),
levels = c(-1:-5,-10), cex=2)
# draw a cross to indicate the maximum
points(M1$par[1], M1$par[2],pch=3)
# draw the -1.92 line(circle) on the contour map
coutour.line <- contourLines(b, sigma, rel.log.like.value, levels = -1.92)[[1]]
lines(coutour.line$x, coutour.line$y, col = 'red', lty = 2, lwd = 2)
# draw the -1.92 line(circle) on the contour map
coutour.line <- contourLines(b, sigma, rel.log.like.value, levels = -1.92)[[1]]
##########################
########################## back to the rabbit data
# optim() for two-dimensional parameter space, b and sigma
# with hessian matrix
result <- optim(par = c(1,1), regression.no.intercept.log.likelihood,
method = "L-BFGS-B",
lower = c(-1000, 0.0001), upper = c(1000,10000),
control = list(fnscale = -1), dat = recap.data, hessian = TRUE)
result$hessian
# by solve() function
var.cov.matrix <- (-1)*solve(result$hessian)
var.cov.matrix
var.cov.matrix # it's the variance-covariance structure fo (b\hat, theta\hat)
